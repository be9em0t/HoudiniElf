{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84af7b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>Import\n",
    "#  Preprocess Python\n",
    "#  LLM processing\n",
    "#  Postprocess Python\n",
    "#  Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ccd92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load INI configuration from lang_pipeline.ini\n",
    "import configparser\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_config(name=\"lang_pipeline.ini\"):\n",
    "    # Search up to 6 levels up then fallback to rglob\n",
    "    p = Path.cwd()\n",
    "    for _ in range(6):\n",
    "        candidate = p / name\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "        p = p.parent\n",
    "    found = next(Path.cwd().rglob(name), None)\n",
    "    return found\n",
    "\n",
    "\n",
    "def _parse_list_option(val):\n",
    "    if not val:\n",
    "        return []\n",
    "    try:\n",
    "        parsed = ast.literal_eval(val)\n",
    "        if isinstance(parsed, (list, tuple)):\n",
    "            return [str(x).strip() for x in parsed]\n",
    "    except Exception:\n",
    "        return [c.strip() for c in str(val).split(',') if c.strip()]\n",
    "    return []\n",
    "\n",
    "\n",
    "cfg_path = find_config()\n",
    "CONFIG = {}\n",
    "if cfg_path:\n",
    "    cp = configparser.ConfigParser()\n",
    "    cp.read(cfg_path)\n",
    "    if 'pipeline' in cp:\n",
    "        sec = cp['pipeline']\n",
    "        CONFIG['source'] = sec.get('source', None)\n",
    "        CONFIG['source_browser'] = sec.getboolean('source_browser', fallback=True)\n",
    "        CONFIG['preview'] = sec.getboolean('preview', fallback=True)\n",
    "        CONFIG['preview_rows'] = sec.getint('preview_rows', fallback=20)\n",
    "        CONFIG['fix_nbsp'] = sec.getboolean('fix_nbsp', fallback=True)\n",
    "        CONFIG['transliterate'] = sec.getboolean('transliterate', fallback=False)\n",
    "        CONFIG['normalize'] = sec.getboolean('normalize', fallback=False)\n",
    "        CONFIG['custom_prompt'] = sec.get('custom_prompt', '')\n",
    "        CONFIG['chunk_size'] = sec.getint('chunk_size', fallback=10000)\n",
    "        CONFIG['columns_preprocess'] = _parse_list_option(sec.get('columns_preprocess', sec.get('columns_process', '')))\n",
    "        CONFIG['columns_agent'] = _parse_list_option(sec.get('columns_agent', ''))\n",
    "        CONFIG['columns_postprocess'] = _parse_list_option(sec.get('columns_postprocess', ''))\n",
    "    print(f\"Loaded config from: \\n{cfg_path}\")\n",
    "else:\n",
    "    print(\"No lang_pipeline.ini found; using defaults\")\n",
    "\n",
    "\n",
    "# Back-compat variables for convenience\n",
    "SOURCE_BROWSER = CONFIG.get('source_browser', True)\n",
    "PREVIEW_ENABLED = CONFIG.get('preview', True)\n",
    "PREVIEW_ROWS = CONFIG.get('preview_rows', 20)\n",
    "COLUMNS_PREPROCESS = CONFIG.get('columns_preprocess', ['translation'])\n",
    "COLUMNS_AGENT = CONFIG.get('columns_agent', [])\n",
    "COLUMNS_POSTPROCESS = CONFIG.get('columns_postprocess', ['translation'])\n",
    "\n",
    "# Lightweight pandas display settings (mitigate heavy HTML rendering in Jupyter/Electron/Chrome)\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.html.table_schema\", False)\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"display.max_columns\", 20)\n",
    "# Force plain-text notebook repr to reduce CPU and memory usage\n",
    "pd.set_option(\"display.notebook_repr_html\", False)\n",
    "# Also set a conservative width for column display\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68148f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File selection (interactive or from INI)\n",
    "import platform, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# If config forces using source directly, use it\n",
    "if not globals().get('SOURCE_BROWSER', True) and CONFIG.get('source'):\n",
    "    input_path = Path(CONFIG['source'])\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"Configured (INI) source not found: {input_path}\")\n",
    "    print(\"Configured (INI) source used:\", input_path)\n",
    "else:\n",
    "    # Try existing picker; otherwise fallback to OS-specific implementation\n",
    "    try:\n",
    "        from tools.ui_native import pick_file\n",
    "    except Exception:\n",
    "        def pick_file(filter_str=None):\n",
    "            system = platform.system()\n",
    "            if system == \"Darwin\":\n",
    "                script = '''\n",
    "                set theFile to choose file\n",
    "                POSIX path of theFile\n",
    "                '''\n",
    "                res = subprocess.run([\"osascript\", \"-e\", script], capture_output=True, text=True)\n",
    "                return res.stdout.strip()\n",
    "            elif system == \"Windows\":\n",
    "                ps_script = r'''\n",
    "                Add-Type -AssemblyName System.Windows.Forms\n",
    "                $ofd = New-Object System.Windows.Forms.OpenFileDialog\n",
    "                $ofd.Filter = \"All files (*.*)|*.*\"\n",
    "                if ($ofd.ShowDialog() -eq \"OK\") { Write-Output $ofd.FileName }\n",
    "                '''\n",
    "                res = subprocess.run([\"powershell\", \"-NoProfile\", \"-Command\", ps_script], capture_output=True, text=True)\n",
    "                return res.stdout.strip()\n",
    "            else:\n",
    "                raise NotImplementedError(\"No native file dialog for this OS\")\n",
    "\n",
    "    pick_path = pick_file(\"CSV files (*.csv)|*.csv\")\n",
    "    if not pick_path:\n",
    "        raise FileNotFoundError(\"No file selected\")\n",
    "\n",
    "    input_path = Path(pick_path)\n",
    "    print(f\"Selected input file: \\n{input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line-by-line import with malformed-line filtering\n",
    "import csv\n",
    "\n",
    "rows = []\n",
    "bad_rows = []\n",
    "total_lines = 0\n",
    "with input_path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\", quotechar='\"', escapechar='\\\\')\n",
    "    try:\n",
    "        header = next(reader)\n",
    "    except StopIteration:\n",
    "        header = []\n",
    "    for line_number, line in enumerate(reader, start=2):\n",
    "        total_lines += 1\n",
    "        if len(line) == len(header):\n",
    "            rows.append(dict(zip(header, line)))\n",
    "        elif len(line) > len(header) and len(line) % len(header) == 0:\n",
    "            for i in range(0, len(line), len(header)):\n",
    "                subline = line[i:i+len(header)]\n",
    "                rows.append(dict(zip(header, subline)))\n",
    "        else:\n",
    "            bad_rows.append((line_number, line))\n",
    "\n",
    "# Summary and fallout storage\n",
    "expected_columns = header if 'header' in locals() else []\n",
    "print(f\"Header: \\n{expected_columns}\\n\") \n",
    "\n",
    "print(f\"Imported {len(rows)} rows ({total_lines} lines read).\")\n",
    "if bad_rows:\n",
    "    # fallout file has same extension and stem with _fallout appended\n",
    "    fallout_path = input_path.with_name(f\"{input_path.stem}_fallout{input_path.suffix}\")\n",
    "\n",
    "    # Re-read original file and write raw lines corresponding to bad line numbers\n",
    "    bad_line_numbers = {ln for ln, _ in bad_rows}\n",
    "    with input_path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as src, fallout_path.open(\"w\", encoding=\"utf-8\", errors=\"replace\") as out:\n",
    "        # write header for context if available\n",
    "        if header:\n",
    "            out.write('\\t'.join(header) + '\\n')\n",
    "        for ln, raw in enumerate(src, start=1):\n",
    "            if ln in bad_line_numbers:\n",
    "                out.write(raw)\n",
    "\n",
    "    print(f\"{len(bad_rows)} malformed lines found. \\nMalformed lines stored in: \\n{fallout_path}\")\n",
    "else:\n",
    "    print(\"No malformed lines found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef22207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of imported lines\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "if globals().get('PREVIEW_ENABLED', True):\n",
    "    print(f\"Preview (first {PREVIEW_ROWS} rows):\")\n",
    "    # Print a lightweight plain-text preview to avoid heavy HTML rendering\n",
    "    print(df.head(PREVIEW_ROWS).to_string(index=False))\n",
    "else:\n",
    "    print(\"Preview disabled by INI (PREVIEW_ENABLED=False)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a431f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import\n",
    "#>>Preprocess Python\n",
    "#  LLM processing\n",
    "#  Postprocess Python\n",
    "#  Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba7a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the expected set of columns from the header\n",
    "expected_columns = list(rows[0].keys())\n",
    "\n",
    "bad_rows = []\n",
    "for i, row in enumerate(rows):\n",
    "    if set(row.keys()) != set(expected_columns):\n",
    "        bad_rows.append((i, row))\n",
    "\n",
    "if bad_rows:\n",
    "    print(\"Bad rows:\")\n",
    "    for idx, row in bad_rows:\n",
    "        print(f\"Row {idx}: {row}\")\n",
    "else:\n",
    "    print(\"No bad rows found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude bad rows from further processing\n",
    "bad_rows_list = []\n",
    "good_rows = []\n",
    "for i, row in enumerate(rows):\n",
    "    if set(row.keys()) != set(expected_columns):\n",
    "        bad_rows_list.append((i, row))\n",
    "    else:\n",
    "        good_rows.append(row)\n",
    "\n",
    "if bad_rows_list:\n",
    "    print(\"Excluding bad rows:\")\n",
    "    for idx, row in bad_rows_list:\n",
    "        print(f\"Row {idx}: {row}\")\n",
    "else:\n",
    "    print(\"No bad rows to exclude.\")\n",
    "\n",
    "rows = good_rows\n",
    "print(f\"Proceeding with {len(rows)} good rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb2191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure language column is correct as means of verifying the imported data\n",
    "languages = {row.get(\"language\") for row in rows}\n",
    "primary_lang = next(iter(languages)) # pick the first\n",
    "languages\n",
    "if len(languages) > 1:\n",
    "    print(\"Warning: multiple language values found:\", languages)\n",
    "else:\n",
    "    print(f\"Primary language: {primary_lang}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac59998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix spaces and NBSPs\n",
    "import re\n",
    "\n",
    "# Configuration for space normalization\n",
    "NPC_AS_SPACE = [\"\\u00A0\"]  # Characters to treat as spaces\n",
    "\n",
    "# Build regex patterns\n",
    "NPC_AS_SPACE_CLASS = r'\\u00A0'  # Since only one char\n",
    "SEQ_SPACES_NPC = re.compile(r'(?:[ ]|' + NPC_AS_SPACE_CLASS + r')+')\n",
    "MULTI_SPACES = re.compile(r'[ ]{2,}')\n",
    "NPC_RUN = re.compile(r'(?:' + NPC_AS_SPACE_CLASS + r'){1,}')\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Normalize spaces and non-printable characters in text.\"\"\"\n",
    "    if s is None:\n",
    "        return s\n",
    "    # Replace any sequence of ASCII spaces and NBSP with a single ASCII space\n",
    "    s = SEQ_SPACES_NPC.sub(' ', s)\n",
    "    # Replace any remaining runs of NBSP with a single ASCII space\n",
    "    s = NPC_RUN.sub(' ', s)\n",
    "    # Collapse 2+ ASCII spaces to one\n",
    "    s = MULTI_SPACES.sub(' ', s)\n",
    "    return s\n",
    "\n",
    "# Columns to process (driven by config 'columns_preprocess'; defaults to ['translation'])\n",
    "# COLUMNS_PREPROCESS is loaded from lang_pipeline.ini if present\n",
    "columns_to_fix = COLUMNS_PREPROCESS if 'COLUMNS_PREPROCESS' in globals() else ['translation']\n",
    "# Support special string values like ['all'] to indicate 'process all columns'\n",
    "if isinstance(columns_to_fix, list) and len(columns_to_fix) == 1 and str(columns_to_fix[0]).lower() == 'all':\n",
    "    columns_to_fix = list(expected_columns) if expected_columns else []\n",
    "# Fallback safety\n",
    "if not columns_to_fix:\n",
    "    columns_to_fix = ['translation']\n",
    "\n",
    "changed_count = 0\n",
    "for row in rows:\n",
    "    for col in columns_to_fix:\n",
    "        if col in row:\n",
    "            original = row[col]\n",
    "            new = normalize_text(row[col])\n",
    "            row[col] = new\n",
    "            if new != original:\n",
    "                changed_count += 1\n",
    "\n",
    "print(f\"Normalized {changed_count} cells in columns: {columns_to_fix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7074cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview table (lightweight)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "if globals().get('PREVIEW_ENABLED', True):\n",
    "    print(f\"Preview (first {PREVIEW_ROWS} rows):\")\n",
    "    # Use plain-text to_string to avoid HTML rendering; cap at PREVIEW_ROWS rows\n",
    "    print(df.head(PREVIEW_ROWS).to_string(index=False))\n",
    "else:\n",
    "    print(\"Preview disabled by INI (PREVIEW_ENABLED=False)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2551f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import\n",
    "#  Preprocess Python\n",
    "#>>LLM processing\n",
    "#  Postprocess Python\n",
    "#  Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM processing placeholder\n",
    "def llm_transform(records):\n",
    "    \"\"\"\n",
    "    Placeholder for LLM transformation.\n",
    "    Input: list of dicts\n",
    "    Output: list of dicts (same schema or extended)\n",
    "    \"\"\"\n",
    "    # TODO: replace with actual LLM call\n",
    "    return records\n",
    "\n",
    "processed_rows = llm_transform(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e4e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import\n",
    "#  Preprocess Python\n",
    "#  LLM processing\n",
    "#>>Postprocess Python\n",
    "#  Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize CSV:\n",
    "# - make cells proper strings\n",
    "# - make dict cells into json\n",
    "\n",
    "def normalize_for_csv(row):\n",
    "    out = {}\n",
    "    for k, v in row.items():\n",
    "        if isinstance(v, (dict, list)):\n",
    "            out[k] = json.dumps(v, ensure_ascii=False)\n",
    "        else:\n",
    "            out[k] = \"\" if v is None else str(v)\n",
    "    return out\n",
    "\n",
    "processed_rows = [normalize_for_csv(r) for r in processed_rows]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3092d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude columns from export\n",
    "\n",
    "# To be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da110674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export result\n",
    "output_suffix = \"_out\"\n",
    "\n",
    "# Extract stem (filename without extension) and extension\n",
    "stem = input_path.stem          # \"input\"\n",
    "ext = input_path.suffix         # \".csv\"\n",
    "\n",
    "# Build new filename\n",
    "output_path = input_path.with_name(f\"{stem}{output_suffix}{ext}\")\n",
    "print(f\"Output to {output_path}\")\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=expected_columns, delimiter=\"\\t\")\n",
    "    writer.writeheader()\n",
    "    writer.writerows(processed_rows)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
