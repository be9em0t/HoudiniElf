{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "84af7b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>Import\n",
    "#  Preprocess Python\n",
    "#  LLM processing\n",
    "#  Postprocess Python\n",
    "#  Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "81ccd92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from: \n",
      "/Users/dunevv/WorkLocal/_AI_/HoudiniElf/tools_Didka/Notebooks/lang_pipeline.ini\n"
     ]
    }
   ],
   "source": [
    "# Load INI configuration from lang_pipeline.ini\n",
    "import configparser\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_config(name=\"lang_pipeline.ini\"):\n",
    "    # Search up to 6 levels up then fallback to rglob\n",
    "    p = Path.cwd()\n",
    "    for _ in range(6):\n",
    "        candidate = p / name\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "        p = p.parent\n",
    "    found = next(Path.cwd().rglob(name), None)\n",
    "    return found\n",
    "\n",
    "\n",
    "def _parse_list_option(val):\n",
    "    if not val:\n",
    "        return []\n",
    "    try:\n",
    "        parsed = ast.literal_eval(val)\n",
    "        if isinstance(parsed, (list, tuple)):\n",
    "            return [str(x).strip() for x in parsed]\n",
    "    except Exception:\n",
    "        return [c.strip() for c in str(val).split(',') if c.strip()]\n",
    "    return []\n",
    "\n",
    "\n",
    "cfg_path = find_config()\n",
    "CONFIG = {}\n",
    "if cfg_path:\n",
    "    cp = configparser.ConfigParser()\n",
    "    cp.read(cfg_path)\n",
    "    if 'pipeline' in cp:\n",
    "        sec = cp['pipeline']\n",
    "        CONFIG['source'] = sec.get('source', None)\n",
    "        CONFIG['source_browser'] = sec.getboolean('source_browser', fallback=True)\n",
    "        CONFIG['preview'] = sec.getboolean('preview', fallback=True)\n",
    "        CONFIG['preview_rows'] = sec.getint('preview_rows', fallback=20)\n",
    "        CONFIG['fix_nbsp'] = sec.getboolean('fix_nbsp', fallback=True)\n",
    "        CONFIG['transliterate'] = sec.getboolean('transliterate', fallback=False)\n",
    "        CONFIG['normalize'] = sec.getboolean('normalize', fallback=False)\n",
    "        CONFIG['custom_prompt'] = sec.get('custom_prompt', '')\n",
    "        CONFIG['chunk_size'] = sec.getint('chunk_size', fallback=10000)\n",
    "        CONFIG['columns_preprocess'] = _parse_list_option(sec.get('columns_preprocess', sec.get('columns_process', '')))\n",
    "        CONFIG['columns_agent'] = _parse_list_option(sec.get('columns_agent', ''))\n",
    "        CONFIG['columns_postprocess'] = _parse_list_option(sec.get('columns_postprocess', ''))\n",
    "        # New export-related options\n",
    "        CONFIG['columns_export_filter'] = sec.getboolean('columns_export_filter', fallback=False)\n",
    "        CONFIG['columns_export'] = _parse_list_option(sec.get('columns_export', ''))\n",
    "        # Validation options\n",
    "        CONFIG['columns_validate_active'] = sec.getboolean('columns_validate_active', fallback=False)\n",
    "        CONFIG['columns_validate_content'] = _parse_list_option(sec.get('columns_validate_content', ''))\n",
    "    print(f\"Loaded config from: \\n{cfg_path}\")\n",
    "else:\n",
    "    print(\"No lang_pipeline.ini found; using defaults\")\n",
    "\n",
    "\n",
    "# Back-compat variables for convenience\n",
    "SOURCE_BROWSER = CONFIG.get('source_browser', True)\n",
    "PREVIEW_ENABLED = CONFIG.get('preview', True)\n",
    "PREVIEW_ROWS = CONFIG.get('preview_rows', 20)\n",
    "COLUMNS_PREPROCESS = CONFIG.get('columns_preprocess', ['translation'])\n",
    "COLUMNS_AGENT = CONFIG.get('columns_agent', [])\n",
    "COLUMNS_POSTPROCESS = CONFIG.get('columns_postprocess', ['translation'])\n",
    "# Export settings\n",
    "COLUMNS_EXPORT_FILTER = CONFIG.get('columns_export_filter', False)\n",
    "COLUMNS_EXPORT = CONFIG.get('columns_export', [])\n",
    "# Validation settings\n",
    "COLUMNS_VALIDATE_ACTIVE = CONFIG.get('columns_validate_active', False)\n",
    "COLUMNS_VALIDATE_CONTENT = CONFIG.get('columns_validate_content', [])\n",
    "\n",
    "# # Lightweight pandas display settings (mitigate heavy HTML rendering in Jupyter/Electron/Chrome)\n",
    "# import pandas as pd\n",
    "# pd.set_option(\"display.html.table_schema\", False)\n",
    "# pd.set_option(\"display.max_rows\", 20)\n",
    "# pd.set_option(\"display.max_columns\", 20)\n",
    "# # Force plain-text notebook repr to reduce CPU and memory usage\n",
    "# pd.set_option(\"display.notebook_repr_html\", False)\n",
    "# # Also set a conservative width for column display\n",
    "# pd.set_option(\"display.width\", 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "68148f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured (INI) source used: /Users/dunevv/WorkLocal/_AI_/HoudiniElf/tools_Didka/test_files/sources/Process_names_LLMoutput_numbered.csv\n"
     ]
    }
   ],
   "source": [
    "# File selection (interactive or from INI)\n",
    "import platform, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# If config forces using source directly, use it\n",
    "if not globals().get('SOURCE_BROWSER', True) and CONFIG.get('source'):\n",
    "    input_path = Path(CONFIG['source'])\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"Configured (INI) source not found: {input_path}\")\n",
    "    print(\"Configured (INI) source used:\", input_path)\n",
    "else:\n",
    "    # Try existing picker; otherwise fallback to OS-specific implementation\n",
    "    try:\n",
    "        from tools.ui_native import pick_file\n",
    "    except Exception:\n",
    "        def pick_file(filter_str=None):\n",
    "            system = platform.system()\n",
    "            if system == \"Darwin\":\n",
    "                script = '''\n",
    "                set theFile to choose file\n",
    "                POSIX path of theFile\n",
    "                '''\n",
    "                res = subprocess.run([\"osascript\", \"-e\", script], capture_output=True, text=True)\n",
    "                return res.stdout.strip()\n",
    "            elif system == \"Windows\":\n",
    "                ps_script = r'''\n",
    "                Add-Type -AssemblyName System.Windows.Forms\n",
    "                $ofd = New-Object System.Windows.Forms.OpenFileDialog\n",
    "                $ofd.Filter = \"All files (*.*)|*.*\"\n",
    "                if ($ofd.ShowDialog() -eq \"OK\") { Write-Output $ofd.FileName }\n",
    "                '''\n",
    "                res = subprocess.run([\"powershell\", \"-NoProfile\", \"-Command\", ps_script], capture_output=True, text=True)\n",
    "                return res.stdout.strip()\n",
    "            else:\n",
    "                raise NotImplementedError(\"No native file dialog for this OS\")\n",
    "\n",
    "    pick_path = pick_file(\"CSV files (*.csv)|*.csv\")\n",
    "    if not pick_path:\n",
    "        raise FileNotFoundError(\"No file selected\")\n",
    "\n",
    "    input_path = Path(pick_path)\n",
    "    print(f\"Selected input file: \\n{input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aebe0131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: \n",
      "['ID', 'name', 'language', 'translation', 'confidence', 'breakdown', 'Remaining', 'Unallowed_characters', 'transliterations', 'Unmapped_in_transliterions']\n",
      "\n",
      "Imported 277576 rows (277586 lines read).\n",
      "10 malformed lines found. \n",
      "Malformed lines stored in: \n",
      "/Users/dunevv/WorkLocal/_AI_/HoudiniElf/tools_Didka/test_files/sources/Process_names_LLMoutput_numbered_fallout.csv\n"
     ]
    }
   ],
   "source": [
    "# Line-by-line import with malformed-line filtering\n",
    "import csv\n",
    "\n",
    "rows = []\n",
    "bad_rows = []\n",
    "total_lines = 0\n",
    "with input_path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\", quotechar='\"', escapechar='\\\\')\n",
    "    try:\n",
    "        header = next(reader)\n",
    "    except StopIteration:\n",
    "        header = []\n",
    "    for line_number, line in enumerate(reader, start=2):\n",
    "        total_lines += 1\n",
    "        if len(line) == len(header):\n",
    "            rows.append(dict(zip(header, line)))\n",
    "        elif len(line) > len(header) and len(line) % len(header) == 0:\n",
    "            for i in range(0, len(line), len(header)):\n",
    "                subline = line[i:i+len(header)]\n",
    "                rows.append(dict(zip(header, subline)))\n",
    "        else:\n",
    "            bad_rows.append((line_number, line))\n",
    "\n",
    "# Summary and fallout storage\n",
    "expected_columns = header if 'header' in locals() else []\n",
    "print(f\"Header: \\n{expected_columns}\\n\") \n",
    "\n",
    "print(f\"Imported {len(rows)} rows ({total_lines} lines read).\")\n",
    "if bad_rows:\n",
    "    # fallout file has same extension and stem with _fallout appended\n",
    "    fallout_path = input_path.with_name(f\"{input_path.stem}_fallout{input_path.suffix}\")\n",
    "\n",
    "    # Re-read original file and write raw lines corresponding to bad line numbers\n",
    "    bad_line_numbers = {ln for ln, _ in bad_rows}\n",
    "    with input_path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as src, fallout_path.open(\"w\", encoding=\"utf-8\", errors=\"replace\") as out:\n",
    "        # write header for context if available\n",
    "        if header:\n",
    "            out.write('\\t'.join(header) + '\\n')\n",
    "        for ln, raw in enumerate(src, start=1):\n",
    "            if ln in bad_line_numbers:\n",
    "                out.write(raw)\n",
    "\n",
    "    print(f\"{len(bad_rows)} malformed lines found. \\nMalformed lines stored in: \\n{fallout_path}\")\n",
    "else:\n",
    "    print(\"No malformed lines found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9e4dca4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation active. Required columns: ['ID', 'name', 'language', 'translation']\n",
      "Validation failed for 277576 rows.\n",
      "Columns missing (counts):\n",
      " - ID: 277576 rows\n",
      "First 5 failing rows (index -> missing columns):\n",
      " Row 0: ['ID']\n",
      " Row 1: ['ID']\n",
      " Row 2: ['ID']\n",
      " Row 3: ['ID']\n",
      " Row 4: ['ID']\n"
     ]
    }
   ],
   "source": [
    "# Validate required columns (from INI)\n",
    "# If COLUMNS_VALIDATE_ACTIVE is True, ensure every row has non-empty values for listed columns\n",
    "if globals().get('COLUMNS_VALIDATE_ACTIVE', False):\n",
    "    required = COLUMNS_VALIDATE_CONTENT or []\n",
    "    print(f\"Validation active. Required columns: {required}\")\n",
    "\n",
    "    if not required:\n",
    "        print(\"No columns specified in 'columns_validate_content'; skipping validation.\")\n",
    "    else:\n",
    "        # Warn about columns that aren't present in the detected header\n",
    "        missing_cols = [c for c in required if c not in expected_columns]\n",
    "        if missing_cols:\n",
    "            print(f\"Warning: these columns specified in 'columns_validate_content' were not found and will be skipped: {missing_cols}\")\n",
    "\n",
    "        validate_cols = [c for c in required if c in expected_columns]\n",
    "        if not validate_cols:\n",
    "            print(\"No valid columns to validate after filtering; skipping validation.\")\n",
    "        else:\n",
    "            # Collect rows that are missing one or more required columns\n",
    "            rows_with_missing = []\n",
    "            for i, row in enumerate(rows):\n",
    "                # treat None or all-whitespace as missing\n",
    "                empty = [c for c in validate_cols if not str(row.get(c, '')).strip()]\n",
    "                if empty:\n",
    "                    rows_with_missing.append((i, empty))\n",
    "\n",
    "            if rows_with_missing:\n",
    "                total = len(rows_with_missing)\n",
    "                # Aggregate missing column counts\n",
    "                from collections import Counter\n",
    "                col_counts = Counter()\n",
    "                for _, empties in rows_with_missing:\n",
    "                    for c in empties:\n",
    "                        col_counts[c] += 1\n",
    "\n",
    "                print(f\"Validation failed for {total} rows.\")\n",
    "                print(\"Columns missing (counts):\")\n",
    "                for c, cnt in col_counts.most_common():\n",
    "                    print(f\" - {c}: {cnt} rows\")\n",
    "\n",
    "                # Show a few examples\n",
    "                N = 5\n",
    "                print(f\"First {min(N, total)} failing rows (index -> missing columns):\")\n",
    "                for idx, empties in rows_with_missing[:N]:\n",
    "                    print(f\" Row {idx}: {empties}\")\n",
    "\n",
    "                # Exclude invalid rows from further processing\n",
    "                invalid_indexes = {idx for idx, _ in rows_with_missing}\n",
    "                rows = [r for i, r in enumerate(rows) if i not in invalid_indexes]\n",
    "\n",
    "            else:\n",
    "                print(\"All rows passed validation.\")\n",
    "\n",
    "else:\n",
    "    print(\"Column content validation disabled (COLUMNS_VALIDATE_ACTIVE=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8ef22207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview (first 20 rows):\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Preview of imported lines\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "if globals().get('PREVIEW_ENABLED', True):\n",
    "    print(f\"Preview (first {PREVIEW_ROWS} rows):\")\n",
    "    # Print a lightweight plain-text preview to avoid heavy HTML rendering\n",
    "    print(df.head(PREVIEW_ROWS).to_string(index=False))\n",
    "else:\n",
    "    print(\"Preview disabled by INI (PREVIEW_ENABLED=False)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "13a431f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import\n",
    "#>>Preprocess Python\n",
    "#  LLM processing\n",
    "#  Postprocess Python\n",
    "#  Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7ba7a17c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Determine the expected set of columns from the header\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m expected_columns = \u001b[38;5;28mlist\u001b[39m(\u001b[43mrows\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.keys())\n\u001b[32m      4\u001b[39m bad_rows = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(rows):\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Determine the expected set of columns from the header\n",
    "expected_columns = list(rows[0].keys())\n",
    "\n",
    "bad_rows = []\n",
    "for i, row in enumerate(rows):\n",
    "    if set(row.keys()) != set(expected_columns):\n",
    "        bad_rows.append((i, row))\n",
    "\n",
    "if bad_rows:\n",
    "    print(\"Bad rows:\")\n",
    "    for idx, row in bad_rows:\n",
    "        print(f\"Row {idx}: {row}\")\n",
    "else:\n",
    "    print(\"No bad rows found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude bad rows from further processing\n",
    "bad_rows_list = []\n",
    "good_rows = []\n",
    "for i, row in enumerate(rows):\n",
    "    if set(row.keys()) != set(expected_columns):\n",
    "        bad_rows_list.append((i, row))\n",
    "    else:\n",
    "        good_rows.append(row)\n",
    "\n",
    "if bad_rows_list:\n",
    "    print(\"Excluding bad rows:\")\n",
    "    for idx, row in bad_rows_list:\n",
    "        print(f\"Row {idx}: {row}\")\n",
    "else:\n",
    "    print(\"No bad rows to exclude.\")\n",
    "\n",
    "rows = good_rows\n",
    "print(f\"Proceeding with {len(rows)} good rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb2191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure language column is correct as means of verifying the imported data\n",
    "languages = {row.get(\"language\") for row in rows}\n",
    "primary_lang = next(iter(languages)) # pick the first\n",
    "languages\n",
    "if len(languages) > 1:\n",
    "    print(\"Warning: multiple language values found:\", languages)\n",
    "else:\n",
    "    print(f\"Primary language: {primary_lang}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac59998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix spaces and NBSPs\n",
    "import re\n",
    "\n",
    "# Configuration for space normalization\n",
    "NPC_AS_SPACE = [\"\\u00A0\"]  # Characters to treat as spaces\n",
    "\n",
    "# Build regex patterns\n",
    "NPC_AS_SPACE_CLASS = r'\\u00A0'  # Since only one char\n",
    "SEQ_SPACES_NPC = re.compile(r'(?:[ ]|' + NPC_AS_SPACE_CLASS + r')+')\n",
    "MULTI_SPACES = re.compile(r'[ ]{2,}')\n",
    "NPC_RUN = re.compile(r'(?:' + NPC_AS_SPACE_CLASS + r'){1,}')\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Normalize spaces and non-printable characters in text.\"\"\"\n",
    "    if s is None:\n",
    "        return s\n",
    "    # Replace any sequence of ASCII spaces and NBSP with a single ASCII space\n",
    "    s = SEQ_SPACES_NPC.sub(' ', s)\n",
    "    # Replace any remaining runs of NBSP with a single ASCII space\n",
    "    s = NPC_RUN.sub(' ', s)\n",
    "    # Collapse 2+ ASCII spaces to one\n",
    "    s = MULTI_SPACES.sub(' ', s)\n",
    "    return s\n",
    "\n",
    "# Columns to process (driven by config 'columns_preprocess'; defaults to ['translation'])\n",
    "# COLUMNS_PREPROCESS is loaded from lang_pipeline.ini if present\n",
    "columns_to_fix = COLUMNS_PREPROCESS if 'COLUMNS_PREPROCESS' in globals() else ['translation']\n",
    "# Support special string values like ['all'] to indicate 'process all columns'\n",
    "if isinstance(columns_to_fix, list) and len(columns_to_fix) == 1 and str(columns_to_fix[0]).lower() == 'all':\n",
    "    columns_to_fix = list(expected_columns) if expected_columns else []\n",
    "# Fallback safety\n",
    "if not columns_to_fix:\n",
    "    columns_to_fix = ['translation']\n",
    "\n",
    "changed_count = 0\n",
    "for row in rows:\n",
    "    for col in columns_to_fix:\n",
    "        if col in row:\n",
    "            original = row[col]\n",
    "            new = normalize_text(row[col])\n",
    "            row[col] = new\n",
    "            if new != original:\n",
    "                changed_count += 1\n",
    "\n",
    "print(f\"Normalized {changed_count} cells in columns: {columns_to_fix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7074cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview table (lightweight)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "if globals().get('PREVIEW_ENABLED', True):\n",
    "    print(f\"Preview (first {PREVIEW_ROWS} rows):\")\n",
    "    # Use plain-text to_string to avoid HTML rendering; cap at PREVIEW_ROWS rows\n",
    "    print(df.head(PREVIEW_ROWS).to_string(index=False))\n",
    "else:\n",
    "    print(\"Preview disabled by INI (PREVIEW_ENABLED=False)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2551f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import\n",
    "#  Preprocess Python\n",
    "#>>LLM processing\n",
    "#  Postprocess Python\n",
    "#  Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM processing placeholder\n",
    "def llm_transform(records):\n",
    "    \"\"\"\n",
    "    Placeholder for LLM transformation.\n",
    "    Input: list of dicts\n",
    "    Output: list of dicts (same schema or extended)\n",
    "    \"\"\"\n",
    "    # TODO: replace with actual LLM call\n",
    "    return records\n",
    "\n",
    "processed_rows = llm_transform(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e4e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import\n",
    "#  Preprocess Python\n",
    "#  LLM processing\n",
    "#>>Postprocess Python\n",
    "#  Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize CSV:\n",
    "# - make cells proper strings\n",
    "# - make dict cells into json\n",
    "\n",
    "def normalize_for_csv(row):\n",
    "    out = {}\n",
    "    for k, v in row.items():\n",
    "        if isinstance(v, (dict, list)):\n",
    "            out[k] = json.dumps(v, ensure_ascii=False)\n",
    "        else:\n",
    "            out[k] = \"\" if v is None else str(v)\n",
    "    return out\n",
    "\n",
    "processed_rows = [normalize_for_csv(r) for r in processed_rows]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3092d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude columns from export\n",
    "# Compute EXPORT_FIELDNAMES based on INI settings and detected columns\n",
    "EXPORT_FIELDNAMES = list(expected_columns) if 'expected_columns' in globals() else []\n",
    "\n",
    "try:\n",
    "    if globals().get('COLUMNS_EXPORT_FILTER', False):\n",
    "        cols = COLUMNS_EXPORT or []\n",
    "        # Support a single token 'all' to mean export all columns\n",
    "        if len(cols) == 1 and str(cols[0]).lower() == 'all':\n",
    "            EXPORT_FIELDNAMES = list(expected_columns)\n",
    "        else:\n",
    "            # Preserve order from COLUMNS_EXPORT but only keep columns that exist\n",
    "            EXPORT_FIELDNAMES = [c for c in cols if c in expected_columns]\n",
    "            missing = [c for c in cols if c not in expected_columns]\n",
    "            if missing:\n",
    "                print(f\"Warning: these columns from 'columns_export' were not found and will be skipped: {missing}\")\n",
    "        if not EXPORT_FIELDNAMES:\n",
    "            print(\"Warning: no valid export columns found; exporting all columns instead.\")\n",
    "            EXPORT_FIELDNAMES = list(expected_columns)\n",
    "    else:\n",
    "        EXPORT_FIELDNAMES = list(expected_columns)\n",
    "except Exception as e:\n",
    "    print(f\"Error determining export columns: {e}. Exporting all columns.\")\n",
    "    EXPORT_FIELDNAMES = list(expected_columns)\n",
    "\n",
    "print(f\"Exporting columns: {EXPORT_FIELDNAMES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da110674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export result\n",
    "output_suffix = \"_out\"\n",
    "\n",
    "# Extract stem (filename without extension) and extension\n",
    "stem = input_path.stem          # \"input\"\n",
    "ext = input_path.suffix         # \".csv\"\n",
    "\n",
    "# Build new filename\n",
    "output_path = input_path.with_name(f\"{stem}{output_suffix}{ext}\")\n",
    "print(f\"Output to {output_path}\")\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    # Use computed EXPORT_FIELDNAMES so export can be filtered via INI\n",
    "    # Allow extra keys in rows (ignore them) and write '' for missing fields\n",
    "    writer = csv.DictWriter(f, fieldnames=EXPORT_FIELDNAMES, delimiter=\"\\t\", extrasaction='ignore', restval='')\n",
    "    writer.writeheader()\n",
    "    writer.writerows(processed_rows)\n",
    "\n",
    "print(\"Export complete\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
